<body>


<div class="container-fluid main-container">




<div id="header">



<h1 class="title toc-ignore">Jackknife Variance Estimator for Multiply
Imputed Response Variables Under Uncongeniality: A Monte Carlo
Simulation Study</h1>
<h4 class="author">Ihsan E. Buker &amp; Samantha R. Seals</h4>
<h4 class="date">2022-08-02</h4>

</div>


<div id="background" class="section level1">
<h1>Background</h1>
<p>Missing data is an issue ubiquitous in many fields of science. Given
that statistical procedures often require data to be fully observed,
there has long been an interest and need to manage missing data in a way
that allows researchers to reach valid conclusions. Today, multiple
imputation is accepted as the gold standard in missing data analysis, in
no short thanks to the work of Donald Rubin <span class="citation">[@buuren_flexible_2012]</span>. In 1977, Rubin proposed
using multiple completed versions of the dataset with missing
observations, applying the complete-data procedure of interest, and
pooling the estimates to draw valid inferences <span class="citation">[@buuren_flexible_2012]</span>. The main advantage of
multiple imputation, as opposed to single imputation, which had been
used by researchers since the early 1970s, is its ability to properly
estimate the variance caused by missing observations <span class="citation">[@Efron_1994]</span>. The emphasis placed on variance
and uncertainty by Rubin was a departure from the status quo of the
time, which was to fill in the missing observation with the most
probable value and to proceed with complete case analyses as if the
observation had not been missing, to begin with <span class="citation">[@buuren_flexible_2012]</span>. This approach, however,
fails to incorporate the loss of information caused by missing
observations into the estimation of parameters, resulting in the
underestimation of variance <span class="citation">[@rubin1978multiple]</span>.</p>
<p>Like all revolutionary ideas, multiple imputation received harsh
criticism following its conception. Perhaps the most notable of the
objections came from Fay in 1992, who demonstrated through
counterexamples that multiple imputation produced biased covariance
estimates <span class="citation">[@bartlett_bootstrap_2020]</span>. Fay
added the need for unison between the imputation and analysis model made
multiple imputation a poor general-purpose tool, particularly in
instances where the imputer and analyst are different individuals <span class="citation">[@fay1992inferences; @buuren_flexible_2012]</span>.
Fay’s arguments led to the conceptualization of congeniality between the
imputation and analysis model, which was later accepted to be a
requirement to obtain valid inferences from multiple imputation using
Rubin’s pooling rules <span class="citation">[@buuren_flexible_2012;
@meng_multiple-imputation_1994]</span>. Although Fay’s work initially
criticized biases introduced to the covariance matrix following multiple
imputation, a similar phenomenon of biased estimations were observed
with variance under uncongeniality <span class="citation">[@fay1992inferences; @meng_multiple-imputation_1994;
@xie_dissecting_2016]</span>.</p>
<p>Some of the earliest works demonstrating Rubin’s variance estimator
to be biased under uncongeniality were from Wang_1998 and Robins in
1998, who also proposed an alternate variance estimator in the same
paper <span class="citation">[@buuren_flexible_2012]</span>. The
variance estimator proposed by Wang and Robins requires the calculation
of several quantities, which are not easily accessible to the average
statistical practitioner <span class="citation">[@bartlett_bootstrap_2020]</span>. The challenging
construction of the variance estimator proposed by Wang and Robins has
resulted in it receiving little-to-no attention in applied settings
<span class="citation">[@bartlett_bootstrap_2020]</span>. In an attempt
to create a more user-friendly variance estimator in instances of
suspected uncongeniality, researchers have proposed combining resampling
methods with multiple imputation. Of the two main resampling methods,
bootstrap has received more attention from multiple imputation
researchers compared to jackknife resampling, which has mostly been
investigated under single hot-deck imputation. Although particular
combinations of bootstrap and multiple imputation have been demonstrated
to create asymptotically unbiased estimates of variance, the associated
computational cost makes this an active area of research <span class="citation">[@bartlett_bootstrap_2020]</span>. Most recently, von
Hippel has proposed a bootstrap variance estimator which addresses the
issue of computational cost; however, it has been demonstrated to create
confidence intervals that are slightly wider compared to traditional
bootstrap and multiple imputation combinations <span class="citation">[@bartlett_bootstrap_2020]</span>. Given the lower
computational cost associated with jackknife resampling, as well as
desirable properties demonstrated under single imputation, such as being
unbiased in certain scenarios, it is an attractive alternative that
should be considered as a variance estimator of multiply imputed data
under uncongeniality <span class="citation">[@chen_jackknife_2001;
@rao_jackknife_1992]</span>.</p>
</div>
<div id="research-goals" class="section level1">
<h1>Research Goals</h1>
<div id="long-term" class="section level2">
<h2>Long-Term</h2>
<p>The long-term goal of this research project is to develop a jackknife
variance estimator that can provide asymptotically unbiased estimates
under uncongeniality at a reasonable computational cost, namely, one
that can easily be attained in applied settings. The long-term goals of
this research project may be particularized by the following set of
questions presented.</p>
<ul>
<li><p>What are the properties of the jackknife variance estimator in
multiply imputed datasets, namely, datasets containing missing outcome
variables imputed through predictive mean matching?</p></li>
<li><p>In what manner should the jackknife estimator be combined with
multiple imputation to obtain an estimator with desirable
characteristics?</p></li>
<li><p>How do the characteristics of the response variable, such as the
proportion of missingness, and the number of observations influence the
performance of the proposed jackknife variance estimator?</p></li>
<li><p>How does the proposed jackknife variance estimator compare to
Rubin’s rules, and bootstrap resampling methods in estimating the
variance of multiply imputed variables under uncongeniality?</p></li>
<li><p>If any bias is noted in the variance estimates obtained through
the jackknife estimator, are they negligible given the decreased
computational cost?</p></li>
</ul>
</div>
<div id="short-term" class="section level2">
<h2>Short-Term</h2>
<p>The short-term goal of this research project was to examine various
characteristics of robust variance estimation methods in the literature,
such as the bias of the point estimator, confidence interval width,
coverage probability, and computational cost. The methods examined draw
from Bayesian and frequentist frameworks to construct various estimators
using bootstrap resampling combined with multiple imputation. The
short-term goals of this project may be particularized by the following
set of questions presented.</p>
<ul>
<li><p>How does the quality of the point estimator obtained by different
methods vary under differing sample sizes, percentage of missing data,
and congeniality?</p></li>
<li><p>How do the characteristics of the confidence interval obtained by
different methods vary under differing sample sizes, percentage of
missing data, and congeniality?</p></li>
<li><p>How does the computational expense of the different methods
compare to one another, and can they realistically be performed in
applied settings?</p></li>
</ul>
</div>
</div>
<div id="research-methods" class="section level1">
<h1>Research Methods</h1>
<p>For the proposed Monte Carlo simulation, <span class="math inline">\(N\)</span> = 999 datasets will be generated with
the following characteristics: A response variable, <span class="math inline">\(Y\)</span>, with <span class="math inline">\(p_{miss} \in \{0.10, 0.30, 0.50\}\)</span>
proportion of missing observations, where the mechanism of missingness
is missing at random (MAR), and an <span class="math inline">\(n \times
q\)</span> matrix of fully observed covariates, where <span class="math inline">\(n \in \{500, 1000, 10000\}\)</span> is the sample
size, and <span class="math inline">\(q = 3\)</span> is the number of
covariates. Moreover, two of the covariates will be simulated such that
there is an interaction between them.During the imputation phase of the
study, the uncongenial imputation model will ignore such interaction,
whereas the congenial imputation model will accommodate it by imputing
the data in two different parts.</p>
<p>Each of the <span class="math inline">\(N\)</span> datasets generated
with the foregoing characteristics will be imputed using the <span class="math inline">\(\texttt{mice()}\)</span> function. In order to
determine <span class="math inline">\(m\)</span>, which represents the
number of complete datasets to be generated per <span class="math inline">\(N\)</span>, von Hippel’s two-stage <span class="math inline">\(m\)</span> calculator will be utilized <span class="citation">[@von_Hippel_2020]</span>. The imputation process will
be visually monitored for convergence using a pilot study with small
<span class="math inline">\(N\)</span>, which will help determine the
maximum number of iterations each combination of <span class="math inline">\(N\)</span> and <span class="math inline">\(m\)</span> are allowed to go through. Finally, the
imputation method for the outcome variable will be predictive mean
matching.</p>
<p>Initially, the imputation model utilized by <span class="math inline">\(\texttt{mice()}\)</span> will be uncongenial to
the analysis model, per the definition of congeniality set forth in
Appendix B. Thereafter, the analysis model of interest to estimate <span class="math inline">\(\theta\)</span> will be applied to each of the
<span class="math inline">\(m\)</span> datasets, and <span class="math inline">\(\text{var}(\hat{\theta})\)</span> will be
estimated using the jackknife estimator. The foregoing process will be
repeated under a congenial imputation and analysis model pairing, which
will serve as the control. All analyses will be conducted using <span class="math inline">\(\texttt{R}\)</span> 4.2.1 (Funny-Looking Kid),
alongside <span class="math inline">\(\texttt{mice}\)</span> 3.14.0,
<span class="math inline">\(\texttt{rsample}\)</span> 1.0.0, <span class="math inline">\(\texttt{bootImpute}\)</span> 1.2.0, and <span class="math inline">\(\texttt{tidyverse}\)</span> 1.3.1.</p>
</div>
<div id="results" class="section level1">
<h1>Results</h1>
<div id="general-observations" class="section level2">
<h2>General observations</h2>
<ul>
<li>Somewhat poor C.I coverage across all methods and conditions.
<ul>
<li>High(er) M.C standard error (~1.2%) due to 10,000 iterations vs. 333
iterations.</li>
<li>Sub-optimal conditions (B = 5 vs. B = 200, m = 5 vs. m = 10)</li>
</ul></li>
<li>Increased sample size leads to narrower confidence intervals.</li>
<li>Increased percentage of missing data leads to wider confidence
intervals</li>
<li>Increased sample size leads to higher coverage probability</li>
<li>Compared to Rubin’s Rules, bootstrap methods have more accurate
point estimates robust to changes in sample size.</li>
<li>Bootstrap MI pooled had the highest coverage probability (~89.2% for
n = 1000) under uncongeniality</li>
<li>Bootstrap MI pooled had the least accurate point estimate under
uncongeniality, but improved as n increased.</li>
<li>Rubin’s Method had nearly no C.I coverage at high percentages of
missing data, even under congeniality.</li>
<li>Rubin’s Method is precise but inaccurate at high percentages of
missing data.</li>
</ul>
</div>
</div>
<div id="discussion" class="section level1">
<h1>Discussion</h1>
</div>
<div id="references" class="section level1">
<h1>References</h1>
<div id="refs">

</div>
</div>
<div id="appendix" class="section level1">
<h1>Appendix</h1>
<div id="data" class="section level2">
<h2>Data</h2>
</div>
<div id="notation" class="section level2">
<h2>Notation</h2>
<ul>
<li><span class="math inline">\(N\)</span> is the total number of units
in the finite population being targeted.<br />
</li>
<li><span class="math inline">\(X\)</span> is an <span class="math inline">\(N \times q\)</span> matrix of fully observed
covariates.<br />
</li>
<li><span class="math inline">\(Y\)</span> is an <span class="math inline">\(N \times p\)</span> matrix of partially observed
outcome variables.<br />
</li>
<li><span class="math inline">\(R\)</span> is an <span class="math inline">\(N \times p\)</span> matrix of response indicators
(i.e., <span class="math inline">\(R_{ij} = 1\)</span> if the response
on <span class="math inline">\(Y_{ij}\)</span> is obtained and <span class="math inline">\(R_{ij} = 0\)</span> otherwise.)<br />
</li>
<li><span class="math inline">\(Q\)</span> is an unknown quantity of
interest to the analyst.</li>
<li><span class="math inline">\(Z_c = \{X, Y_{inc}\}\)</span> is the
complete data.</li>
<li><span class="math inline">\(Z_o = \{X, Y_{obs}, R_{inc}\}\)</span>
is the incomplete (i.e., observed) data.</li>
<li>The analyst’s complete-data procedure is summarized by <span class="math inline">\(\mathscr{P}_{com} = [\hat{Q}(X, Y_{inc}), U(X,
Y_{inc})]\)</span>, where <span class="math inline">\(\hat{Q}(X,
Y_{inc})\)</span> is an estimator of <span class="math inline">\(Q\)</span> with associated variance <span class="math inline">\(U(X, Y_{inc}.)\)</span></li>
<li><span class="math inline">\(R\)</span> is not a part of <span class="math inline">\(\mathscr{P}_{com}\)</span>, as the missing at
random assumption implies that the response behavior itself carries no
information about <span class="math inline">\(Q\)</span>.</li>
</ul>
</div>
<div id="formal-definition-of-congeniality" class="section level2">
<h2>Formal Definition of Congeniality</h2>
<p>In short, one may define congeniality as the imputer and analyst
making different assumptions regarding the data. The following two-part
formal definition of uncongeniality was proposed by Meng in 1994, and
will be utilized in our research. Meeting the assumptions set forth in
the following two definitions qualifies the imputation model as being
congenial to the analysis model, or vice versa.</p>
<div id="definition-1" class="section level3">
<h3>Definition 1</h3>
<p>Let <span class="math inline">\(E_f\)</span> and <span class="math inline">\(V_f\)</span> denote posterior mean and variance
with respect to <span class="math inline">\(f\)</span>, respectively. A
Bayesian model <span class="math inline">\(f\)</span> is said to be
congenial to the analysis procedure <span class="math inline">\(\mathscr{P} \equiv \{\mathscr{P}_{obs};
\mathscr{P}_{com}\}\)</span> for given <span class="math inline">\(Z_o\)</span> if the following hold:</p>
<ul>
<li><p>The posterior mean and variance of <span class="math inline">\(\theta\)</span> under <span class="math inline">\(f\)</span> given the incomplete data are
asymptotically the same as the estimate and variance from the analyst’s
incomplete-data procedure <span class="math inline">\(\mathscr{P}_{obs}\)</span>, that is, <span class="math display">\[\begin{equation}
    [\hat{\theta}(Z_o), U(Z_o)] \simeq [E_f[\theta | Z_o], V_f[\theta |
Z_o]]
\end{equation}\]</span></p></li>
<li><p>The posterior mean and variance of <span class="math inline">\(\theta\)</span> under <span class="math inline">\(f\)</span> given the complete data are
asymptotically the same as the estimate and variance from the analyst’s
complete-data procedure <span class="math inline">\(\mathscr{P}_{com}\)</span>, that is, <span class="math display">\[\begin{equation}
     [\hat{\theta}(Z_c), U(Z_c)] \simeq [E_f[\theta | Z_c], V_f[\theta |
Z_c]]
\end{equation}\]</span></p>
<p>for any possible <span class="math inline">\(Y_{inc} = (Y_{obs},
Y_{miss})\)</span> with <span class="math inline">\(Y_{obs}\)</span>
conditioned upon.</p></li>
</ul>
<p>If the foregoing conditions are met, <span class="math inline">\(f\)</span> is said to be second-moment congenial
to <span class="math inline">\(\mathscr{P}\)</span>.</p>
</div>
<div id="definition-2" class="section level3">
<h3>Definition 2</h3>
<p>The analysis procedure <span class="math inline">\(\mathscr{P}\)</span> is said to be congenial to
the imputation model <span class="math inline">\(g(Y_{miss}|Z_o,
A)\)</span> where <span class="math inline">\(A\)</span> represents
possible additional data the imputer has access to, if one can find an
<span class="math inline">\(f\)</span> such that (<strong>i</strong>)
<span class="math inline">\(f\)</span> is congenial to <span class="math inline">\(\mathscr{P}\)</span> and (<strong>ii</strong>) the
posterior predictive density for <span class="math inline">\(Y_{miss}\)</span> derived under <span class="math inline">\(f\)</span> is identical to the imputation model
<span class="math inline">\(f(Y_{miss}|Z_o) = g(Y_{miss}|Z_o, A) \
\forall \ Y_{miss}\)</span>.</p>
</div>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
