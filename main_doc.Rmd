---
title: "main_doc"
author: "Ihsan E. Buker"
date: "`r Sys.Date()`"
output: html_document
---
# Dependencies 
```{r}
library(tidyverse)
library(magrittr)
library(mice)
library(MASS)

library(boot)
library(rsample) # rsample might give more flexibility with mids. 
```

# Toy Example 
```{r, cache=T}
set.seed(971423)

cor_mat <- matrix(c(1, 0.5, 0.5, 
                    0.5, 1, 0.5, 
                    0.5, 0.5, 1), nrow = 3, ncol = 3)

mean_vec <- c(1, 5, 10)

data_complete <- mvrnorm(100, mu = mean_vec, Sigma = cor_mat)

data_w_missing <- ampute(data_complete, prop = 0.3, mech = "MAR", patterns = c(0, 1, 1))$amp

# V1 is an outcome variable with 30% MAR. Covariates are fully observed. 

# The analysis model is V1 ~ V2 + V3 + epsilon 
# We are interested in estimating var(beta*V2)

# Congenial imputation model 
congenial_imputation_model <- make.predictorMatrix(data_w_missing) %>%
  as.matrix()

# Uncongenial imputation model 
uncongenial_imputation_model <- make.predictorMatrix(data_w_missing) %>%
  as.matrix()

uncongenial_imputation_model[1, ] <- c(0,1,0)

# Congenial imputation 
con_imp <- mice(data_w_missing, method = "pmm", predictorMatrix = congenial_imputation_model)

# Uncongenial imputation 
uncon_imp <- mice(data_w_missing, method = "pmm", predictorMatrix = uncongenial_imputation_model)

# Fitting analysis model 

# Rubin's rules 
# var(beta*V2) = 0.02020468 -> Our reference value. 
con_model <- with(con_imp, lm(V1 ~ V2 + V3)) %>%
  pool() %>%
  summary() %>%
  mutate(variance = std.error^2)

# var(beta*V2) = 0.01320681
uncon_model <- with(uncon_imp, lm(V1 ~ V2 + V3)) %>%
  pool() %>%
  summary() %>%
  mutate(variance = std.error^2)

# MI boot Rubin 
# Creates "times" bootstrap samples for each m imputed dataset. 
# From there, applies model to each of the m_i_b_j bootstraps

times = 1E3
bootstrap_samples <- map(
  complete(uncon_imp, "long") %>%
    group_split(.imp),
  ~ bootstraps(.,
               times = times,
               apparent = FALSE) %>%
    mutate(
      model = map(splits, ~ lm(V1 ~ V2 + V3, data = .)),
      coef_inf = map(model, tidy)
    )
)

# Function to obtain the statistics associated with term==V2, and calculate var.  
coef_extractor <- function(x){
  model_coefs <- x %>%
    unnest(coef_inf) %>%
    filter(term == "V2") %>%
    mutate(variance = std.error ^ 2)
  
  return(model_coefs)
}

model_coefs <- map(bootstrap_samples, coef_extractor)

m_mean_vars <- sapply(model_coefs, function(x) mean(x$variance)) 

model_coefs_w_mean <- mapply(cbind, model_coefs, "m_mean_var" = m_mean_vars, SIMPLIFY = F)

# Function to calculate numerator of the variance formula 
var_calc <- function(x){
  x %>%
    dplyr::mutate("internal_argument" = ({variance - m_mean_vars})^2)
}

model_coefs_w_numerator <- map(model_coefs_w_mean, var_calc)

ID <- c(1:(uncon_imp$m))

model_coefs_final <- mapply(cbind, model_coefs_w_numerator, "SampleID"=ID, SIMPLIFY=F)

flat_list <- bind_rows(model_coefs_final)

flat_list %>%
  ggplot(aes(
    x = variance,
    y = ..scaled..,
    color = as.factor(SampleID),
    fill = as.factor(SampleID)
  )) +
  geom_density(alpha = .3) +
  ggtitle(paste("Distribution of Estimated Variance of B_{V2}: B =", times, sep = "")) +
  xlab("Estimate") +
  ylab("Frequency") +
  theme_minimal() +
  theme(legend.title = element_blank())

p <- flat_list %>%
  dplyr::select(SampleID, internal_argument) %>%
  group_split(SampleID) 

sapply(p, function(x) sum(x$internal_argument)) %>%
  mean()

# MI boot pooled percentile
se <- function(x) sqrt(var(x) / length(x))

# var(theta_2) = 0.009357518
var_est_theta_2 <- sapply(p, function(x) sum(x$internal_argument)) %>%
  mean()

# se(theta_2) = 0.001376763
se_theta_2 <- sapply(p, function(x) sum(x$internal_argument)) %>%
  se()

UB <- var_est_theta_2 + 1.96*se_theta_2
LB <- var_est_theta_2 - 1.96*se_theta_2

print(c(UB, LB))

# Boot MI percentile

# 1) B bootstraps are generated from the observed data. 
# 2) Each of the b=1...B datasets are imputed M times. 
# 3) Rubin's rules are used to obtain point estimates of theta
# 4) Bootstrap confidence interval of the estimator is obtained
library(bootImpute)

boot_impute <- bootMice(data_w_missing, nBoot = 100, nImp = 2, nCores = 4, seed = 971423)

x <- map(boot_impute, ~tidy(lm(V1 ~ V2 + V3, data = .)))

res <- c()
for(i in 1:length(x)){
  res[[i]] <- x[[i]] %>%
  dplyr::filter(term=="V2") %>%
  mutate(variance = std.error^2)
}

flat_models <- bind_rows(res, .id = "column_label")

flat_models$variance %>%
  mean()

lm(V1 ~ V2 + V3, data = as.data.frame(data_complete)) %>%
  summary()
```

# Rubin's method with the uncongenial imputation model is more accurate than with the congenial imputation model, how? 

# Why is the Boot MI percentile severely (1/10) underestimating the variance compared to other methods? 

# 